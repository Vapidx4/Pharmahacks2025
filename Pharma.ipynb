{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSLgqKvLPlB",
        "outputId": "b1c753d5-d068-4cb5-9ae1-160e2ad9b8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.12.2)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=b85c10c20f0f305e656cdafb312f6bd8a6e88077ab3e9fc4add18845c3fea364\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown tabulate\n",
        "!pip install typing-extensions --upgrade\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "iouGFoR3NRRv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import copy\n",
        "import gdown\n",
        "import os\n",
        "from tabulate import tabulate\n",
        "import tensorflow as tf\n",
        "# import tf_agents\n",
        "# from tf_agents.environments import suite_gym\n",
        "# from tf_agents.environments import tf_py_environment\n",
        "# from tf_agents.agents.dqn import dqn_agent\n",
        "# from tf_agents.networks.q_network import QNetwork\n",
        "# from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "# from tf_agents.utils import common\n",
        "# from tf_agents.drivers import dynamic_step_driver\n",
        "# from tf_agents.policies import random_tf_policy\n",
        "# from tf_agents.trajectories import trajectory\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5xhyfmrNTAr",
        "outputId": "b50a6092-bd4b-465e-e359-6ed7fb768f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data_hackaton_train.data already exists. Skipping download.\n",
            "--2025-03-16 17:24:43--  https://github.com/devpatel30/amr/raw/main/test_data.pickle\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/devpatel30/amr/main/test_data.pickle [following]\n",
            "--2025-03-16 17:24:44--  https://raw.githubusercontent.com/devpatel30/amr/main/test_data.pickle\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24524011 (23M) [application/octet-stream]\n",
            "Saving to: ‘test_data.pickle’\n",
            "\n",
            "test_data.pickle    100%[===================>]  23.39M  39.1MB/s    in 0.6s    \n",
            "\n",
            "2025-03-16 17:24:45 (39.1 MB/s) - ‘test_data.pickle’ saved [24524011/24524011]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define file path\n",
        "file_path = \"data_hackaton_train.data\"\n",
        "\n",
        "# Check if file exists before downloading\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {file_path}...\")\n",
        "    gdown.download('https://drive.google.com/uc?id=18HCzpRnUzMYwrWJ4k6qnkgqhlLY735NO', file_path, quiet=False)\n",
        "else:\n",
        "    print(f\"{file_path} already exists. Skipping download.\")\n",
        "train_df = pd.read_pickle('/content/data_hackaton_train.data')\n",
        "\n",
        "url = 'https://github.com/devpatel30/amr/raw/main/test_data.pickle'\n",
        "\n",
        "# Download the file\n",
        "!wget {url} -O test_data.pickle\n",
        "\n",
        "global test_data\n",
        "with open('test_data.pickle', 'rb') as f:\n",
        "    test_dat = pd.read_pickle(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO31GrLqNWuN"
      },
      "outputs": [],
      "source": [
        "TARGET_LENGTH = 20\n",
        "\n",
        "# 1) Find the maximum number of sequences across all rows\n",
        "max_num_seqs = max(len(row['start']) for _, row in train_df.iterrows())\n",
        "max_num_seqs = max(max_num_seqs, max(len(row['solution']) for _, row in train_df.iterrows()))\n",
        "\n",
        "for i, row in train_df.iterrows():\n",
        "    # --- Fix the number of sequences in 'start' ---\n",
        "    if len(row['start']) < max_num_seqs:\n",
        "        # pad with sequences of all dashes\n",
        "        needed = max_num_seqs - len(row['start'])\n",
        "        row['start'].extend(['-' * TARGET_LENGTH] * needed)\n",
        "    else:\n",
        "        # if too many sequences, truncate\n",
        "        row['start'] = row['start'][:max_num_seqs]\n",
        "\n",
        "    # --- Fix the number of sequences in 'solution' ---\n",
        "    if len(row['solution']) < max_num_seqs:\n",
        "        needed = max_num_seqs - len(row['solution'])\n",
        "        row['solution'].extend(['-' * TARGET_LENGTH] * needed)\n",
        "    else:\n",
        "        row['solution'] = row['solution'][:max_num_seqs]\n",
        "\n",
        "    # --- Now fix each sequence to length 20 ---\n",
        "    for j, seq in enumerate(row['start']):\n",
        "        if len(seq) < TARGET_LENGTH:\n",
        "            row['start'][j] = seq.ljust(TARGET_LENGTH, '-')\n",
        "        elif len(seq) > TARGET_LENGTH:\n",
        "            row['start'][j] = seq[:TARGET_LENGTH]\n",
        "\n",
        "    for j, seq in enumerate(row['solution']):\n",
        "        if len(seq) < TARGET_LENGTH:\n",
        "            row['solution'][j] = seq.ljust(TARGET_LENGTH, '-')\n",
        "        elif len(seq) > TARGET_LENGTH:\n",
        "            row['solution'][j] = seq[:TARGET_LENGTH]\n",
        "\n",
        "\n",
        "# save first 100 lines as a csv\n",
        "train_df.head(100).to_csv('train_df.csv', index=False)\n",
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfhOJP_2TEJB"
      },
      "outputs": [],
      "source": [
        "train_df\n",
        "\n",
        "# Get row 4 (using 0-based indexing, row 4 is index 4)\n",
        "row = train_df.iloc[4]\n",
        "\n",
        "# Print the number of sequences in start and solution\n",
        "print(\"Row 4 - Number of sequences in 'start':\", len(row['start']))\n",
        "print(\"Row 4 - Number of sequences in 'solution':\", len(row['solution']))\n",
        "\n",
        "# Optionally, print each sequence length for a more detailed look\n",
        "print(\"\\nLengths of each sequence in 'start':\")\n",
        "for idx, seq in enumerate(row['start']):\n",
        "    print(f\"Sequence {idx}: length = {len(seq)}\")\n",
        "\n",
        "print(\"\\nLengths of each sequence in 'solution':\")\n",
        "for idx, seq in enumerate(row['solution']):\n",
        "    print(f\"Sequence {idx}: length = {len(seq)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcaqBwAtTHMk"
      },
      "outputs": [],
      "source": [
        "DICT_SYMBOLS = {\n",
        "    'A': 1,\n",
        "    'T': 2,\n",
        "    'C': 3,\n",
        "    'G': 4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRkQcGfhTJtS"
      },
      "outputs": [],
      "source": [
        "class Puzzle:\n",
        "    def __init__(self, data):\n",
        "        self.start = list(data[\"start\"])\n",
        "        self.moves = copy.deepcopy(data.get(\"moves\"))\n",
        "        self.steps = copy.deepcopy(data.get(\"steps\"))\n",
        "        self.solution = list(data[\"solution\"])\n",
        "        self.score = data[\"score\"]\n",
        "        self.accepted_pair = data[\"accepted_pair\"]\n",
        "\n",
        "        self.padded_start = self.build_puzzle_to_end(self.start)\n",
        "        self.padded_solution = self.build_puzzle_to_end(self.solution)\n",
        "\n",
        "    def build_puzzle_to_end(self, puzzle):\n",
        "        \"\"\"Pad each row with '-' to match the longest row for visualization.\"\"\"\n",
        "        max_len = max(len(row) for row in puzzle)\n",
        "        return [row.ljust(max_len, '-') for row in puzzle]\n",
        "\n",
        "    def plot_puzzle(self, puzzle, title):\n",
        "        puzzle = self.build_puzzle_to_end(puzzle)\n",
        "        num_puzzle = np.array([[DICT_SYMBOLS.get(char, 0) for char in row] for row in puzzle])\n",
        "        rot_num_puzzle = np.rot90(num_puzzle, 1)\n",
        "        puzzle_array = np.array([list(row) for row in puzzle])\n",
        "        rot_labels = np.rot90(puzzle_array, 1)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(rot_num_puzzle, annot=rot_labels, fmt=\"\", cmap=\"Pastel1_r\", cbar=False)\n",
        "        plt.title(title)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    def gearbox_score(self, puzzle, bonus=1.15):\n",
        "        consensus = self.accepted_pair\n",
        "        score = 0\n",
        "        for col_ind in range(len(puzzle[0])):\n",
        "            col_bonus = True\n",
        "            col_tot = 0\n",
        "            column_chars = [row[col_ind] for row in puzzle]\n",
        "            for char in column_chars:\n",
        "                if char == \"-\":\n",
        "                    col_bonus = False\n",
        "                    continue\n",
        "                if char in consensus[col_ind]:\n",
        "                    col_tot += 1\n",
        "                else:\n",
        "                    col_bonus = False\n",
        "            column_score = col_tot * bonus if col_bonus else col_tot\n",
        "            score += column_score\n",
        "        print(f\"Total Gearbox Score: {score}\")\n",
        "        return score\n",
        "\n",
        "    def _apply_step_to_puzzle(self, puzzle, step):\n",
        "                \"\"\"Apply a single step to the puzzle.\"\"\"\n",
        "                new_puzzle = puzzle.copy()\n",
        "                # Convert step elements to integers if they are strings\n",
        "                # Check if step is in the expected format before converting to int\n",
        "                if isinstance(step, list) and len(step) == 2 and step[0].isdigit() and step[1].isdigit():\n",
        "                    row_index = int(step[0]) - 1\n",
        "                    col_index = int(step[1])\n",
        "                else:\n",
        "                    # Handle invalid steps, e.g., skip them or raise a more informative error\n",
        "                    # print(f\"Warning: Invalid step format: {step}. Skipping this step.\")\n",
        "                    return new_puzzle\n",
        "\n",
        "                if row_index < 0 or row_index >= len(new_puzzle):\n",
        "                    return new_puzzle\n",
        "                row_str = new_puzzle[row_index]\n",
        "                if col_index < 0 or col_index > len(row_str):\n",
        "                    return new_puzzle\n",
        "                new_row = row_str[:col_index] + '-' + row_str[col_index:]\n",
        "                new_row = new_row[:len(row_str)]\n",
        "                new_puzzle[row_index] = new_row\n",
        "                return new_puzzle\n",
        "\n",
        "    def apply_all_steps(self):\n",
        "        \"\"\"Apply all steps on a copy of the puzzle and plot states.\"\"\"\n",
        "        current_puzzle = list(self.start)\n",
        "        updated_puzzles = []\n",
        "        scores = []\n",
        "\n",
        "        for step in self.steps:\n",
        "            current_puzzle = self._apply_step_to_puzzle(current_puzzle, step)\n",
        "            padded_current = self.build_puzzle_to_end(current_puzzle)\n",
        "            score = self.gearbox_score(padded_current)\n",
        "            updated_puzzles.append(padded_current)\n",
        "            scores.append(score)\n",
        "\n",
        "        n_steps = len(updated_puzzles)\n",
        "        fig, axes = plt.subplots(1, n_steps, figsize=(4 * n_steps, 6))\n",
        "        if n_steps == 1:\n",
        "            axes = [axes]\n",
        "        for idx, (puzzle_state, score) in enumerate(zip(updated_puzzles, scores)):\n",
        "            num_puzzle = np.array([[DICT_SYMBOLS.get(char, 0) for char in row] for row in puzzle_state])\n",
        "            rot_num_puzzle = np.rot90(num_puzzle, 1)\n",
        "            puzzle_array = np.array([list(row) for row in puzzle_state])\n",
        "            rot_labels = np.rot90(puzzle_array, 1)\n",
        "            ax = axes[idx]\n",
        "            sns.heatmap(rot_num_puzzle, annot=rot_labels, fmt=\"\", cmap=\"Pastel1_r\", cbar=False, ax=ax)\n",
        "            ax.set_title(f\"Step {idx+1}\\nScore: {score}\")\n",
        "            ax.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return current_puzzle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQdldvANTLky"
      },
      "outputs": [],
      "source": [
        "# Create puzzle\n",
        "row = 3\n",
        "\n",
        "#print the row\n",
        "print(tabulate(train_df.iloc[[row]], headers='keys', tablefmt='pretty'))\n",
        "\n",
        "\n",
        "puzzle_data = {\n",
        "    'start': train_df.iloc[row]['start'],\n",
        "    'moves': train_df.iloc[row].get('moves'),\n",
        "    'steps': train_df.iloc[row].get('steps'),\n",
        "    'solution': train_df.iloc[row]['solution'],\n",
        "    'score': train_df.iloc[row]['score'],\n",
        "    'accepted_pair': train_df.iloc[row]['accepted_pair']\n",
        "}\n",
        "\n",
        "puzzle = Puzzle(puzzle_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmE9d7K6TNVh"
      },
      "outputs": [],
      "source": [
        "puzzle.plot_puzzle(puzzle.start, \"Starting Puzzle\")\n",
        "puzzle.plot_puzzle(puzzle.solution, \"Solution Puzzle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkAgILQ2TPLQ"
      },
      "outputs": [],
      "source": [
        "updated_puzzle = puzzle.apply_all_steps()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3myDjGYTRCA"
      },
      "outputs": [],
      "source": [
        "puzzle.steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuK3aOQkmxIz"
      },
      "source": [
        "# Attempt at ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsf-6Ciy_005"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MSADataEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(MSADataEnv, self).__init__()\n",
        "        self.data = data  # use the passed in DataFrame (train_data or val_data)\n",
        "        self.current_index = 0\n",
        "\n",
        "        # Compute the max sequence length from the dataset\n",
        "        all_lengths = []\n",
        "        for idx, row in self.data.iterrows():\n",
        "            for seq_str in row['start']:  # row['start'] is already a list\n",
        "                all_lengths.append(len(seq_str))\n",
        "        self.max_length = max(all_lengths)  # Store max sequence length\n",
        "\n",
        "        self.num_sequences = len(self.data.iloc[0]['start'])  # Count number of sequences in a row\n",
        "\n",
        "        self.gap_insertions = np.zeros((self.num_sequences, self.max_length), dtype=int) # to count the gaps\n",
        "\n",
        "\n",
        "        # Define observation space with fixed max_length\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-1, high=3,\n",
        "            shape=(self.num_sequences, self.max_length),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        # Define the action space: num_sequences * (max_length+1) possible gap insertions\n",
        "        self.total_actions = self.num_sequences * (self.max_length + 1)\n",
        "        self.action_space = spaces.Discrete(self.total_actions)\n",
        "        self.MAX_STEPS = 10  # or another appropriate number\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0  # Reset step counter at the start of an episode\n",
        "        self.current_index = random.randint(0, len(self.data) - 1)\n",
        "        row = self.data.iloc[self.current_index]\n",
        "        self.state = self._process_start(row['start'])\n",
        "        self.target_solution = self._process_solution(row['solution'])\n",
        "        self.accepted_pair = row['accepted_pair']  # Add this line\n",
        "        return self.state.copy()\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        seq_idx = action // (self.max_length + 1)\n",
        "        pos = action % (self.max_length + 1)\n",
        "\n",
        "        sequence = list(self.state[seq_idx])\n",
        "        # Insert gap\n",
        "        sequence.insert(pos, -1)\n",
        "        if len(sequence) > self.max_length:\n",
        "            sequence = sequence[:self.max_length]\n",
        "        self.state[seq_idx] = np.array(sequence)\n",
        "\n",
        "        # *DEBUG PRINT* to see if state is changing\n",
        "        # print(f\"After action {action}: seq {seq_idx}, pos {pos}\")\n",
        "        # for i, row in enumerate(self.state):\n",
        "        #     print(f\" Seq {i}: {row}\")\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.MAX_STEPS\n",
        "        reward = self._calculate_reward(self.state, self.target_solution)\n",
        "\n",
        "        return self.state.copy(), reward, done, {}\n",
        "\n",
        "\n",
        "\n",
        "    def _process_start(self, start_data):\n",
        "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3, '-': -1}\n",
        "        sequences = []\n",
        "        for seq_str in start_data:\n",
        "            numeric_seq = [mapping[ch] for ch in seq_str]\n",
        "            # Pad to length 12\n",
        "            while len(numeric_seq) < self.max_length:\n",
        "                numeric_seq.append(-1)\n",
        "            # Truncate if longer than 12\n",
        "            numeric_seq = numeric_seq[:self.max_length]\n",
        "            sequences.append(numeric_seq)\n",
        "\n",
        "        # If start_data has fewer than self.num_sequences, pad the extra sequences\n",
        "        # If more, truncate. The end result: shape = (self.num_sequences, self.max_length)\n",
        "        if len(sequences) < self.num_sequences:\n",
        "            for _ in range(self.num_sequences - len(sequences)):\n",
        "                sequences.append([-1]*self.max_length)\n",
        "        sequences = sequences[:self.num_sequences]\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _process_solution(self, solution_data):\n",
        "      mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3, '-': -1}\n",
        "      sequences = []\n",
        "      for seq_str in solution_data:\n",
        "          numeric_seq = [mapping[ch] for ch in seq_str]\n",
        "\n",
        "          # Pad up to self.seq_length (e.g., 12)\n",
        "          while len(numeric_seq) < self.max_length:\n",
        "              numeric_seq.append(-1)\n",
        "\n",
        "          # Truncate if it exceeds self.seq_length\n",
        "          numeric_seq = numeric_seq[:self.max_length]\n",
        "\n",
        "          sequences.append(numeric_seq)\n",
        "\n",
        "      return np.array(sequences)\n",
        "\n",
        "    def _calculate_reward(self, state, target):\n",
        "        # Base reward: fraction of matching positions (range: 0 to 1)\n",
        "        return np.mean(state == target)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTJqyQob9Chw"
      },
      "outputs": [],
      "source": [
        "# Define the Q-Network (a simple feed-forward network)\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKIW-FHo9KkR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# Hyperparameters\n",
        "BATCH_SIZE      = 128\n",
        "GAMMA           = 0.99\n",
        "EPS_START       = 0.8\n",
        "EPS_END         = 0.2\n",
        "EPS_DECAY       = 0.999   # approximate exponential decay\n",
        "TARGET_UPDATE   = 128\n",
        "LR              = 1e-4\n",
        "MEMORY_SIZE     = 100000\n",
        "NUM_EPISODES    = 250   # 3250 seems to be best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oRL3QpY9Kac"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your training DataFrame into 80% train and 20% validation\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "#download some data from github and read it\n",
        "\n",
        "\n",
        "\n",
        "# Replay Memory\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Prepare environment and networks\n",
        "env = MSADataEnv(train_data)\n",
        "state_dim = env.num_sequences * env.max_length  # Flatten state\n",
        "action_dim = env.total_actions\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim)\n",
        "target_net = DQN(state_dim, action_dim)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "# Epsilon for epsilon-greedy policy\n",
        "epsilon = EPS_START\n",
        "\n",
        "# Function to select an action\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
        "            q_values = policy_net(state_tensor)\n",
        "            return q_values.max(1)[1].item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "6C9Bj3UueCpy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88dCgPK9HlIJ"
      },
      "outputs": [],
      "source": [
        "reward_history = []\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for episode in range(NUM_EPISODES):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        memory.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        # If we have enough samples, perform optimization\n",
        "        if len(memory) >= BATCH_SIZE:\n",
        "            transitions = memory.sample(BATCH_SIZE)\n",
        "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
        "\n",
        "            batch_state = torch.FloatTensor([s.flatten() for s in batch_state])\n",
        "            batch_action = torch.LongTensor(batch_action).unsqueeze(1)\n",
        "            batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1)\n",
        "            batch_next_state = torch.FloatTensor([s.flatten() for s in batch_next_state])\n",
        "            batch_done = torch.FloatTensor(batch_done).unsqueeze(1)\n",
        "\n",
        "            # Compute current Q values\n",
        "            current_q = policy_net(batch_state).gather(1, batch_action)\n",
        "            # Compute next Q values from target network\n",
        "            next_q = target_net(batch_next_state).max(1)[0].detach().unsqueeze(1)\n",
        "            # Compute expected Q values\n",
        "            expected_q = batch_reward + (GAMMA * next_q * (1 - batch_done))\n",
        "\n",
        "            loss = nn.MSELoss()(current_q, expected_q)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * EPS_DECAY, EPS_END)\n",
        "\n",
        "    # Update target network\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode+1}/{NUM_EPISODES}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai3HKkDx2nXl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "plt.plot(reward_history)  # reward_history = list of total rewards per episode\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Training Progress\")\n",
        "plt.show()\n",
        "\n",
        "smoothed = pd.Series(reward_history).rolling(100).mean()\n",
        "smoothed.plot()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL_gmh9F-tm1"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, agent, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for a specified number of episodes.\n",
        "    Returns the average accuracy and a list of steps for each episode.\n",
        "    \"\"\"\n",
        "    total_accuracy = 0.0\n",
        "    agent.eval()  # Put the agent in evaluation mode\n",
        "    all_episodes_steps = []  # We'll store steps for each episode here\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for episode_idx in range(num_episodes):\n",
        "            # Reset environment\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_steps = []  # This will collect (seq_idx, pos) for the current episode\n",
        "\n",
        "            while not done:\n",
        "                # Let the agent select an action (no exploration in eval)\n",
        "                action = select_action(state, epsilon=0.05)\n",
        "\n",
        "                # Decode the action into (seq_idx, pos) for logging\n",
        "                seq_idx = action // (env.max_length + 1)\n",
        "                pos = action % (env.max_length + 1)\n",
        "                episode_steps.append((seq_idx, pos))\n",
        "\n",
        "                # Step the environment\n",
        "                state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # After the episode finishes, accumulate the reward\n",
        "            total_accuracy += reward\n",
        "\n",
        "            # Store steps for this episode\n",
        "            all_episodes_steps.append(episode_steps)\n",
        "\n",
        "    # Compute average accuracy\n",
        "    avg_accuracy = total_accuracy / num_episodes\n",
        "\n",
        "    return avg_accuracy, all_episodes_steps\n",
        "\n",
        "\n",
        "# Then, call it like so:\n",
        "env_val = MSADataEnv(test_dat)\n",
        "accuracy, episodes_steps = evaluate_agent(env_val, policy_net, num_episodes=10)\n",
        "\n",
        "print(f\"Average Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "def remove_duplicates(steps):\n",
        "    seen = set()\n",
        "    unique_steps = []\n",
        "    for step in steps:\n",
        "        if step not in seen:\n",
        "            unique_steps.append(step)\n",
        "            seen.add(step)\n",
        "    return unique_steps\n",
        "\n",
        "unique_episodes_steps = [remove_duplicates(steps) for steps in episodes_steps]\n",
        "\n",
        "# Print out the steps for each episode\n",
        "for i, step_list in enumerate(unique_episodes_steps):\n",
        "    print(f\"Episode {i}: {step_list}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract the row you want\n",
        "row_4 = train_df.iloc[[1]]  # Notice the double brackets so it's still a DataFrame\n",
        "\n",
        "# 2. Create an environment with just this row\n",
        "env_test_row4 = MSADataEnv(row_4)\n",
        "\n",
        "# 3. Evaluate your agent on that single row\n",
        "accuracy_row4, steps_row4 = evaluate_agent(env_test_row4, policy_net, num_episodes=1)\n",
        "\n",
        "print(f\"Accuracy for row 4: {accuracy_row4 * 100:.2f}%\")\n",
        "print(\"Steps taken (row 4):\", steps_row4[0])  # steps_row4 is a list of lists; we only have 1 episode\n"
      ],
      "metadata": {
        "id": "WykTcoCgxflu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gearbox_score(accepted_pair, puzzle, bonus=1.15):\n",
        "    \"\"\"\n",
        "    Calculate and print the total gearbox score based on a puzzle grid and accepted character pairs.\n",
        "\n",
        "    The function iterates through each column of the puzzle, summing up occurrences of characters that\n",
        "    are present in the consensus (accepted_pair) for that specific column. For each column, if every\n",
        "    non-dash character matches the accepted consensus, a bonus multiplier is applied to the column score.\n",
        "\n",
        "    Parameters:\n",
        "        accepted_pair (list of iterables): Each element contains the accepted characters for the\n",
        "                                             corresponding column in the puzzle.\n",
        "        puzzle (list of list of str): A 2D grid representing the puzzle, where each inner list is a row.\n",
        "        bonus (float, optional): The bonus multiplier applied to a column's score if all characters (except dashes)\n",
        "                                 match the accepted consensus. Defaults to 1.15.\n",
        "\n",
        "    Returns:\n",
        "        float: The total gearbox score calculated by summing the column scores, with bonuses applied as appropriate.\n",
        "    \"\"\"\n",
        "    consensus = accepted_pair\n",
        "    score = 0\n",
        "    for col_ind in range(len(puzzle[0])):\n",
        "        col_bonus = True\n",
        "        col_tot = 0\n",
        "        column_chars = [row[col_ind] for row in puzzle]\n",
        "        for char in column_chars:\n",
        "            if char == \"-\":\n",
        "                col_bonus = False\n",
        "                continue\n",
        "            if char in consensus[col_ind]:\n",
        "                col_tot += 1\n",
        "            else:\n",
        "                col_bonus = False\n",
        "        column_score = col_tot * bonus if col_bonus else col_tot\n",
        "        score += column_score\n",
        "    print(f\"Total Gearbox Score: {score}\")\n",
        "    return score\n",
        "\n",
        "#test with row 4\n",
        "gearbox_score(row_4['accepted_pair'].values[0], row_4['start'].values[0])\n"
      ],
      "metadata": {
        "id": "071Zw240yxt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heat Map for the OG data"
      ],
      "metadata": {
        "id": "5KcawSliONHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Extract gap insertions from \"steps\" column\n",
        "# Example format of steps: [(0,3), (1,5), (0,3), (2,1)]\n",
        "gap_positions = []\n",
        "for step_list in train_df['steps']:\n",
        "    # Check if step_list is None before iterating\n",
        "    if step_list is not None:  # Skip if step_list is None\n",
        "        for seq_idx, pos in step_list:\n",
        "            gap_positions.append((seq_idx, pos))\n",
        "\n",
        "# Count most common gap insertions\n",
        "gap_freq = Counter(gap_positions)\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "gap_df = pd.DataFrame(gap_freq.items(), columns=['Position', 'Frequency'])\n",
        "gap_df[['Sequence Index', 'Insertion Position']] = pd.DataFrame(gap_df['Position'].tolist(), index=gap_df.index)\n",
        "gap_df.drop(columns=['Position'], inplace=True)\n",
        "\n",
        "# Plot Heatmap of Frequent Gap Insertions\n",
        "heatmap_data = np.zeros((max(gap_df['Sequence Index'])+1, max(gap_df['Insertion Position'])+1))\n",
        "\n",
        "for _, row in gap_df.iterrows():\n",
        "    heatmap_data[row['Sequence Index'], row['Insertion Position']] = row['Frequency']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(heatmap_data, cmap='Blues', annot=True, fmt=\".0f\")\n",
        "plt.xlabel(\"Insertion Position\")\n",
        "plt.ylabel(\"Sequence Index\")\n",
        "plt.title(\"Gap Insertion Frequency Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aPFD-JqnNnYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying heatmap from the ML"
      ],
      "metadata": {
        "id": "YS_2QrAgOIf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "ytSu7_QtUzJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# def plot_gap_insertions(env):\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     sns.heatmap(env.gap_insertions, cmap=\"Blues\", annot=True, fmt=\".0f\", cbar=True)\n",
        "\n",
        "#     plt.xlabel(\"Position in Sequence\")\n",
        "#     plt.ylabel(\"Sequence Index\")\n",
        "#     plt.title(\"Frequency of Gap Insertions by AI\")\n",
        "#     plt.show()\n",
        "\n",
        "# # After running several episodes\n",
        "# plot_gap_insertions(env)"
      ],
      "metadata": {
        "id": "jG7OUoD3aUSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhMTZGF8cjem"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}